"""
    ContinuousCCEM

ContinuousCCEM implements the Conditional Cross-Entropy Method (CCEM) for policy improvement
[1] with number of samples `n` and action percentiles `Ï` and `ÏÌƒ` for the actor and proposal
policy updates respectively.

## Updates

This section discusses how the actor and proposal policies are updated.

The actor and proposal policies, denoted as `Ï€` and `Ï€Ìƒ` respectively, use a CCEM update for
policy improvement. The CCEM update works as follows:
    1. Set hyperparameters `n`, `Ï`, `ÏÌƒ`
    2. Generate a set of `n` actions ğ”¸
    3. Order that set of actions using some metric, `Q`. Typically `Q` is an action-value
       function. Denote this set `I = { aâ‚, aâ‚‚, aâ‚ƒ, ... aâ‚™}` with indices such that for
           `i > j, Q(aáµ¢) > Q(aâ±¼)`
    4. Update the actor policy `Ï€` by increasing the log-likelihood of the `âŒŠÏnâŒ‹` actions of
       maximum value under `Q`. The effective gradient is

            I* = { a | Q(a) > Q(a_{âŒŠÏnâŒ‹}) }, where a_{âŒŠÏnâŒ‹} is the Ï-th action percentile
                under Q
            âˆ‡ = ğ”¼_{a âˆ¼ I*} [âˆ‡ln(Ï€(a|s)]

    5. Update the proposal policy `Ï€Ìƒ` by increasing the log-likelihood of the `âŒŠÏÌƒnâŒ‹` actions
       of maximum value under `Q`.

            IÌƒ* = { a | Q(a) > Q(a_{âŒŠÏÌƒnâŒ‹}) }, where a_{âŒŠÏÌƒnâŒ‹} is the ÏÌƒ-th action percentile
                under Q
            âˆ‡ = ğ”¼_{a âˆ¼ IÌƒ*} [âˆ‡ln(Ï€Ìƒ(a|s)]


The set of actions for the CCEM update in (2) above are drawn from the proposal policy as in
[1].

The proposal policy, denoted as Ï€Ìƒ, is updated using a CCEM update as well. To generate
action samples for the CCEM update, Ï€Ìƒ itself is sampled. This sample is shared for updating
both the proposal policy and actor policy.

# References
[1] S. Neumann, Lim, S., Joseph, A., Pan, Y., White, A., White, M. Greedy
Actor-Critic: A New Conditional Cross-Entropy Method for Policy
Improvement. 2022.

[2] Greedification Operators for Policy Optimization: Investigating Forward
and Reverse KL Divergences. Chan, A., Silva H., Lim, S., Kozuno, T.,
Mahmood, A. R., White, M. 2021.

[3] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor. Haarnoja, T., Zhou, A., Abbeel, P.,
Levine, S. International Conference on Machine Learning. 2018.

[4] Soft Actor-Critic: Algorithms and Applications. Haarnoja, T.,
Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V.,
Zhu, H., Gupta, A., Abbeel, P., Levine, S. In preparation. 2019.
"""
struct ContinuousCCEM <: AbstractActorUpdate
    _n::Int
    _Ï::Float32
    _ÏÌƒ::Float32

    # Actor Policy Entropy Regularization
    _Ï„::Float32         # Actor Temperature
    _Ï€_num_entropy_samples::Int

    # Proposal Policy Entropy Regularization
    _Ï„Ìƒ::Float32         # Proposal Temperature
    _Ï€Ìƒ_num_entropy_samples::Int

    function ContinuousCCEM(
        n::Int, Ï::Real, ÏÌƒ::Real, Ï„::Real, Ï€_num_entropy_samples::Int, Ï„Ìƒ::Real,
        Ï€Ìƒ_num_entropy_samples::Int,
    )
        @assert (Ï„ >= 0) "expected Ï„ >= 0"
        @assert (Ï„Ìƒ >= 0) "expected Ï„Ìƒ >= 0"
        @assert (Ï€_num_entropy_samples >= 0) "expected Ï€_num_entropy_samples >= 0"
        @assert (Ï€Ìƒ_num_entropy_samples >= 0) "expected Ï€Ìƒ_num_entropy_samples >= 0"
        @assert (Ï >= 0) "expected Ï >= 0"
        @assert (ÏÌƒ >= 0) "expected ÏÌƒ >= 0"
        @assert (trunc(n * Ï) > 0) "expected âŒŠÏnâŒ‹ > 0"
        @assert (trunc(n * ÏÌƒ) > 0) "expected âŒŠÏÌƒnâŒ‹ > 0"

        return new(n, Ï, ÏÌƒ, Ï„, Ï€_num_entropy_samples, Ï„Ìƒ, Ï€Ìƒ_num_entropy_samples)
    end
end

function setup(
    up::ContinuousCCEM,
    ::AbstractEnvironment,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    rng::AbstractRNG,
)::UpdateState{ContinuousCCEM}
    Ï€Ìƒ = deepcopy(Ï€)
    Ï€Ìƒ_f = deepcopy(Ï€_f)
    Ï€Ìƒ_Î¸ = deepcopy(Ï€_Î¸)
    Ï€Ìƒ_st = deepcopy(Ï€_st)
    Ï€Ìƒ_optim = deepcopy(optim)

    return setup(
        up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Ï€Ìƒ, Ï€Ìƒ_f, Ï€Ìƒ_Î¸, Ï€Ìƒ_st, qf, qf_f, qf_Î¸, qf_st, optim, Ï€Ìƒ_optim,
        rng,
    )
end

function setup(
    up::ContinuousCCEM,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    Ï€Ìƒ::AbstractContinuousParameterisedPolicy,
    Ï€Ìƒ_f,    # proposal policy model
    Ï€Ìƒ_Î¸,    # proposal policy model parameters
    Ï€Ìƒ_st,   # proposal policy model state
    qf::AbstractActionValueFunction,
    qf_f,
    qf_Î¸,
    qf_st,
    Ï€_optim::Optimisers.AbstractRule,
    Ï€Ìƒ_optim::Optimisers.AbstractRule,
    rng::AbstractRNG,
)::UpdateState{ContinuousCCEM}
    return UpdateState(
        up,
        Ï€_optim,
        (
            Ï€_optim = Optimisers.setup(Ï€_optim, Ï€_Î¸),
            Ï€Ìƒ_optim = Optimisers.setup(Ï€Ìƒ_optim, Ï€Ìƒ_Î¸),
            Ï€Ìƒ = Ï€Ìƒ,
            Ï€Ìƒ_f = Ï€Ìƒ_f,
            Ï€Ìƒ_Î¸ = Ï€Ìƒ_Î¸,
            Ï€Ìƒ_st = Ï€Ìƒ_st,
            rng = Lux.replicate(rng)
        )
    )
end

function update(
    st::UpdateState{ContinuousCCEM},
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::Q,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    up = st._update
    rng = Lux.replicate(st._state.rng)

    # Unpack proposal policy from update state
    Ï€Ìƒ = st._state.Ï€Ìƒ
    Ï€Ìƒ_f = st._state.Ï€Ìƒ_f
    Ï€Ìƒ_Î¸ = st._state.Ï€Ìƒ_Î¸
    Ï€Ìƒ_st = st._state.Ï€Ìƒ_st

    # Get the actions of maximal value as ordered by the action-value function critic
    Ï€_top_actions, Ï€Ìƒ_top_actions, Ï€Ìƒ_st = _get_top_actions(
        rng, states, Ï€Ìƒ, Ï€Ìƒ_f, Ï€Ìƒ_Î¸, Ï€Ìƒ_st, qf, qf_f, qf_Î¸, qf_st, up._n, up._Ï, up._ÏÌƒ
    )

    # Compute the actor and proposal gradients using the actions of maximal value gotten
    # above
    âˆ‡Ï€_Î¸, Ï€_st = _gradient(
        up, rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, up._Ï„, up._Ï€_num_entropy_samples, states, Ï€_top_actions,
    )
    âˆ‡Ï€Ìƒ_Î¸, Ï€Ìƒ_st = _gradient(
        up, rng, Ï€Ìƒ, Ï€Ìƒ_f, Ï€Ìƒ_Î¸, Ï€Ìƒ_st, up._Ï„Ìƒ, up._Ï€Ìƒ_num_entropy_samples, states, Ï€Ìƒ_top_actions
    )

    Ï€_optim_state = st._state.Ï€_optim
    Ï€_optim_state, Ï€_Î¸ = Optimisers.update(Ï€_optim_state, Ï€_Î¸, only(âˆ‡Ï€_Î¸))
    Ï€Ìƒ_optim_state = st._state.Ï€Ìƒ_optim
    Ï€Ìƒ_optim_state, Ï€Ìƒ_Î¸ = Optimisers.update(Ï€Ìƒ_optim_state, Ï€Ìƒ_Î¸, only(âˆ‡Ï€Ìƒ_Î¸))

    return UpdateState(
        st._update,
        st._optim,
        (
            Ï€_optim = Ï€_optim_state,
            Ï€Ìƒ_optim = Ï€Ìƒ_optim_state,
            Ï€Ìƒ = Ï€Ìƒ,
            Ï€Ìƒ_f = Ï€Ìƒ_f,
            Ï€Ìƒ_Î¸ = Ï€Ìƒ_Î¸,
            Ï€Ìƒ_st = Ï€Ìƒ_st,
            rng = rng
        )
    ), Ï€_Î¸, Ï€_st, qf_st
end

function _gradient(
    ::ContinuousCCEM,
    rng::AbstractRNG,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,
    Ï€_Î¸,
    Ï€_st,
    Ï„,
    Ï€_num_entropy_samples,
    states::AbstractArray,
    top_actions,
)
    âˆ‡Ï€_Î¸ = if Ï„ > 0 && Ï€_num_entropy_samples > 0
        entropy_samples, Ï€_st = sample(
            Ï€, rng, Ï€_f, Ï€_Î¸, Ï€_st, states; num_samples=Ï€_num_entropy_samples,
        )

        gradient(Ï€_Î¸) do Î¸
            params, Ï€_st = get_params(Ï€, Ï€_f, Î¸, Ï€_st, states)
            lnÏ€ = logprob(Ï€, top_actions, params)

            # Compute entropy regularization
            entropy_lnÏ€ = logprob(Ï€, entropy_samples, params)
            entropy = -(entropy_lnÏ€ .^ 2) ./ 2

            -gpu_mean(lnÏ€) - Ï„ * gpu_mean(entropy)
        end
    else
        gradient(Ï€_Î¸) do Î¸
            lnÏ€, Ï€_st = logprob(Ï€, Ï€_f, Î¸, Ï€_st, states, top_actions)
            -gpu_mean(lnÏ€)
        end
    end

    return âˆ‡Ï€_Î¸, Ï€_st
end

function _get_top_actions(batched_actions, ind)
    top_ind = CartesianIndex.(ind, reshape(LinearIndices(ind[1, :]), 1, :))
    top_actions = @inbounds batched_actions[:, top_ind]
    return top_actions
end

function _get_top_actions(
    rng::AbstractRNG,
    states::AbstractArray,
    Ï€Ìƒ,
    Ï€Ìƒ_f,
    Ï€Ìƒ_Î¸,
    Ï€Ìƒ_st,
    qf,
    qf_f,
    qf_Î¸,
    qf_st,
    n,
    Ï,
    ÏÌƒ,
)
    batch_size = size(states)[end]
    state_size = size(states)[begin:end-1]

    # Sample actions from Ï€Ìƒ for the ContinuousCCEM update
    batched_actions, Ï€Ìƒ_st = sample(
        Ï€Ìƒ,
        rng,
        Ï€Ìƒ_f,
        Ï€Ìƒ_Î¸,
        Ï€Ìƒ_st,
        states;
        num_samples=n,
    )  # (ğ’œ, num_samples, batch)

    action_size = size(batched_actions, 1)
    actions = reshape(batched_actions, action_size..., :)

    # Stack states to calculate action values. Each sampled action requires one state
    # observation
    stacked_states = repeat(
        states;
        inner=(ones(Int, length(state_size))..., n),
    )

    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, stacked_states, actions)
    q = reshape(q, n, batch_size)

    # Calculate how many actions of maximal value to increase the log-likelihood of
    Ï€_n = trunc(Int, Ï * n)
    Ï€Ìƒ_n = trunc(Int, ÏÌƒ * n)

    # Find the indices of actions of maximal value, only sorting the number of actions
    # absolutely required
    n = max(Ï€_n, Ï€Ìƒ_n)
    ixs = if CUDA.functional()
        [CuArray{Int32}(undef, size(q, 1)) for _ in 1:size(q, 2)]
    else
        [Array{Int}(undef, size(q, 1)) for _ in 1:size(q, 2)]
    end
    sortperm!.(ixs, eachcol(q))
    ind = reduce(hcat, ixs)[end-n+1:end, :]

    _top_actions = _get_top_actions(batched_actions, ind)
    Ï€_top_actions, Ï€Ìƒ_top_actions = if Ï€_n > Ï€Ìƒ_n
        _top_actions, _top_actions[:, end-Ï€Ìƒ_n+1:end, :]
    else
        _top_actions[:, end-Ï€_n+1:end, :], _top_actions
    end

    return Ï€_top_actions, Ï€Ìƒ_top_actions, Ï€Ìƒ_st
end
