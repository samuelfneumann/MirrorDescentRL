"""
    ContinuousRKL

ContinuousRKL implements a policy improvement operator which minimizes the reverse
KL-divergence between a learned policy and the Boltzmann distribution over action values
[1]. This operator is equivalent to the policy improvement operator used by SAC [2, 3] when
the constructor arguments are chosen appropriately. This update is more generally an
implementation of the RKL policy improvement operator in [1].

The constructor argument `reparameterised` denotes whether gradients are estimated using the
reparameterisation trick or not. `baseline_actions` determines how many actions to use in
estimating the state-value baselines, and `Ï„` is the temperature parameter. The
`num_samples` keyword determines how many actions are used to estimate the gradient, which
is typically `1`.

## Updates

This section discusses which update targets are used to update the actor and critic, as well
as some implementation details on how these updates are performed.

### Actor Update

For discrete actions, we would use the gradient in equation 6 in [1]:

    âˆ‡RKL(Ï€, â„¬Q) =   ğ”¼_{Ï€} [ Ï„â»Â¹ Q(s, a) - ln(Ï€(a | s))]                                 (1)
                =   -Î£â‚ âˆ‡Ï€(a | s) [ Ï„â»Â¹ Q(s, a) - ln(Ï€(a | s))]                         (2)
where:
    âˆ‡ = âˆ‡_Ï•
    Ï€ = Ï€_Ï•
    Ï„ = temperature argument

For continuous actions, we can no longer sum over all actions, instead, we
use a sampled (estimated) gradient as outlined in equation (10) of [1]:

    âˆ‡RKL(Ï€, â„¬Q) =   ğ”¼_{Ï€} [ [ (Q(s, a) - V(s))(Ï„â»Â¹) - ln(Ï€(aâˆ£s)) ] âˆ‡ln(Ï€(aâˆ£s)) ]        (3)
                â‰ˆ   -1/n  âˆ‘â‚â¿ [ (Q(s, aáµ¢) - V(s))(Ï„â»Â¹) - ln(Ï€(aáµ¢âˆ£s)) ] âˆ‡ln(Ï€(aáµ¢âˆ£s))     (4)
                â‰ˆ   âˆ‡ğ”¼_{Îµ} [ (Q(s, f(Îµ)) - V(s))(Ï„â»Â¹) - ln(Ï€(f(Îµ)âˆ£s)) ]                 (5)
                            where f(Îµ) = a
where:
    âˆ‡ = âˆ‡_Ï•
    Ï€ = Ï€_Ï•
    Ï„ = temperature argument

The gradient (3) is approximated by (4) or (5) using repeated random sampling,
usually using a single action sample aáµ¢. To estimate this gradient, we can use two different
methods. (4) uses the likelihood trick while (5) uses the reparameterization trick.

In the continuous action setting, a state value baseline (V in equations (3-5) above) can be
used. The baseline is approximated as V(s) = ğ”¼[q(s, A)] â‰ˆ 1/n âˆ‘â¿áµ¢â‚Œâ‚ q(s, aáµ¢).

In the discrete action setting, no baseline is required because we can compute the
expectation in equations (1) and (2) (equation (6) in [1]) exactly (assuming the true
value function is known).

### Implementation Details

This implementation of the RKL actor-critic algorithm allows for four different forms of the
RKL update. These are controlled by the fields `_scale_actor_lr_by_temperature` and
`_temperature_scales_entropy`. When `_temperature_scales_entropy`, then the temperature is
treated as an entropy scale as in [1, 4, 5]. When `not _temperature_scales_entropy`, then
the temperature is treated as a reward scale as in [2, 3]. The updates are as follows:

1. `scale_actor_lr_by_temperature and temperature_scales_entropy`:
    - Actor gradient = âˆ‡ğ”¼[Ï„ ln(Ï€) - ğ”¸]

2. `not scale_actor_lr_by_temperature and temperature_scales_entropy`:
    - Actor gradient = âˆ‡ğ”¼[ln(Ï€) - ğ”¸/Ï„]

3. `scale_actor_lr_by_temperature and not temperature_scales_entropy`:
    - Actor gradient = âˆ‡Ï„ğ”¼[ln(Ï€) - ğ”¸]

4. `not scale_actor_lr_by_temperature and not temperature_scales_entropy`:
    - Actor gradient = âˆ‡ğ”¼[ln(Ï€) - ğ”¸]

where ğ”¸ is the advantage function. All these updates are sensible but are implementation
details that can affect the algorithm's performance. The original SAC update in [2, 3] is
equivalent to (4), and this is the implementation used in the original SAC codebase.
The StableBaselines3 codebase [4] and SpinningUp codebase [5] use the implementation in (1).

## References

[1] Greedification Operators for Policy Optimization: Investigating Forward
and Reverse KL Divergences. Chan, A., Silva H., Lim, S., Kozuno, T.,
Mahmood, A. R., White, M. In prepartion. 2021.

[2] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor. Haarnoja, T., Zhou, A., Abbeel, P.,
Levine, S. International Conference on Machine Learning. 2018.

[3] Soft Actor-Critic: Algorithms and Applications. Haarnoja, T.,
Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V.,
Zhu, H., Gupta, A., Abbeel, P., Levine, S. In preparation. 2019.

[4] Stable-Baselines3: Reliable Reinforcement Learning Implementations.
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann,
N. Journal of Machine Learning Research. 2021.

[5] Spinning Up in Deep Reinforcement Learning. Achiam, J. 2018.
"""
struct ContinuousRKL <: AbstractActorUpdate
    _reparam::Bool

    _baseline_actions::Int
    _temperature::Float32
    _num_samples::Int

    function ContinuousRKL(
        reparameterised::Bool, baseline_actions::Int, Ï„::Real, num_samples,
    )
        return new(reparameterised, baseline_actions, Ï„, num_samples)
    end
end

function ContinuousRKL(Ï„; reparam, baseline_actions, num_samples)
    return ContinuousRKL(reparam, baseline_actions, Ï„, num_samples)
end

function setup(
    up::ContinuousRKL,
    ::AbstractEnvironment,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    rng::AbstractRNG,
)::UpdateState{ContinuousRKL}
    @assert up._baseline_actions >= 0 "expected baseline_actions >= 0"
    @assert up._num_samples > 0 "expected num_samples > 0"
    return UpdateState(
        up,
        optim,
        (
            optim = Optimisers.setup(optim, Ï€_Î¸),
            rng = Lux.replicate(rng),
        ),
    )
end

function update(
    st::UpdateState{ContinuousRKL},
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    states::AbstractArray, # Must be >= 2D
)
    up = st._update
    rng = Lux.replicate(st._state.rng)
    âˆ‡Ï€_Î¸, Ï€_st, qf_st = if !up._reparam
        ll_grad(
            up, rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st,states,
        )
    else
        reparam_grad(
            up, rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, states,
        )
    end

    Ï€_optim_state = st._state.optim
    Ï€_optim_state, Ï€_Î¸ = Optimisers.update(Ï€_optim_state, Ï€_Î¸, only(âˆ‡Ï€_Î¸))

    return UpdateState(
        st._update,
        st._optim,
        (
            optim = Ï€_optim_state,
            rng = rng,
        ),
    ), Ï€_Î¸, Ï€_st, qf_st
end

function reparam_grad(
    up::ContinuousRKL,
    rng::AbstractRNG,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    state_batch::AbstractArray, # Must be >= 2D
)
    batch_size = size(state_batch)[end]
    state_size = size(state_batch)[begin:end-1]

    # Estimate the approximate state value for each state using v(s) = ğ”¼[q(s, A)]
    if up._baseline_actions > 0
        v, Ï€_st, qf_st = _get_baseline(
            rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, state_batch,
            up._baseline_actions,
        )
    end

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Î¸
        actions, lnÏ€, Ï€_st = rsample_with_logprob(
            Ï€, rng, Ï€_f, Î¸, Ï€_st, state_batch; num_samples=up._num_samples,
        )
        action_size = size(actions, 1)
        actions = reshape(actions, action_size, :)

        states = repeat(
            state_batch;
            inner = (ones(Int, length(state_size))..., up._num_samples),
        )

        # Compute the advantage
        q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states, actions)
        q = reshape(q, up._num_samples, batch_size)
        adv = up._baseline_actions > 0 ? q .- v : q

        lnÏ€ = reshape(lnÏ€, size(adv))

        # Compute the reparameterized loss
        âˆ‡Ï€ = if up._temperature != 0
            # Soft ContinuousRKL
            @. adv - up._temperature * lnÏ€
        else
            # Hard ContinuousRKL
            adv
        end
        -gpu_mean(âˆ‡Ï€)
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st
end

function ll_grad(
    up::ContinuousRKL,
    rng::AbstractRNG,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    state_batch::AbstractArray, # Must be >= 2D
)
    batch_size = size(state_batch)[end]
    state_size = size(state_batch)[begin:end-1]

    # Estimate the approximate state value for each state using v(s) = ğ”¼[q(s, A)]
    if up._baseline_actions > 0
        v, Ï€_st, qf_st = _get_baseline(
            rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, state_batch,
            up._baseline_actions,
        )
    end

    âˆ‡Ï€ = gradient(Ï€_Î¸) do Î¸
        # Sample actions for each state in the batch, needed to estimate the gradient in
        # each state: âˆ‡J = ğ”¼[q(S, A) - Ï„*ln(Ï€(Aâˆ£S)]
        actions, lnÏ€, Ï€_st = sample_with_logprob(
            Ï€, rng, Ï€_f, Î¸, Ï€_st, state_batch; num_samples=up._num_samples,
        )
        action_size = size(actions, 1)
        actions = reshape(actions, :, batch_size)

        # Repeat states for each action sample in each state
        states = repeat(
            state_batch;
            inner = (ones(Int, length(state_size))..., up._num_samples),
        )

        # Calculate the advantage
        q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states, actions; reduct=true)
        q = reshape(q, up._num_samples, batch_size)
        adv = up._baseline_actions > 0 ? q .- v : q

        lnÏ€ = reshape(lnÏ€, size(adv))

        scale = if up._temperature != 0
            # Soft ContinuousRKL
            @. adv - up._temperature * lnÏ€
        else
            # Hard ContinuousRKL
            adv
        end
        scale = ChainRulesCore.ignore_derivatives(scale)

        -gpu_mean(lnÏ€ .* scale)
    end

    return âˆ‡Ï€, Ï€_st, qf_st
end

function _get_baseline(
    rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, state_batch, num_samples,
)::Tuple{AbstractArray{Float32},<:NamedTuple,<:NamedTuple}
    if num_samples > 0
        batch_size = size(state_batch)[end]
        state_size = size(state_batch)[begin:end-1]

        actions, Ï€_st = sample(
            Ï€, rng, Ï€_f, Ï€_Î¸, Ï€_st, state_batch; num_samples=num_samples,
        )
        action_size = size(actions, 1)
        actions = reshape(actions, action_size..., :)

        states = repeat(
            state_batch;
            inner = (ones(Int, length(state_size))..., num_samples),
        )

        q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states, actions; reduct=true)
        q = reshape(q, num_samples, batch_size)
        return ChainRulesCore.ignore_derivatives(mean(q; dims=1)), Ï€_st, qf_st
    else
        error("cannot compute baseline with 0 actions")
    end
end

