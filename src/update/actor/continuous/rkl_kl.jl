"""
    ContinuousRKLKL

ContinuousRKLKL implements the RKL policy update with a KL penalty
"""
struct ContinuousRKLKL <: AbstractActorUpdate
    _reparam::Bool

    _baseline_actions::Int
    _temperature::Float32
    _num_samples::Int

    _inv_Î»::Float32      # Inverse stepsize for mirror descent (functional) update
    # TODO: this should be renamed to num_kl_updates
    _num_md_updates::Int

    _forward_direction::Bool

    function ContinuousRKLKL(
        reparameterised::Bool, baseline_actions::Int, Ï„::Real, md_Î»::Real,
        num_md_updates::Int, forward_direction::Bool, num_samples::Int,
    )
        @assert (num_md_updates > 1) "expected num_md_updates > 1"
        @assert (num_samples >= 1) "expected num_samples >= 1"
        @assert (md_Î» > 0f0) "expected functional stepsize md_Î» > 0)"
        @assert (Ï„ >= 0) "expected Ï„ >= 0"

        msg = "cannot use reparameterised gradient estimation when forward_direction=true"
        @assert !(forward_direction && reparameterised) msg

        return new(
            reparameterised, baseline_actions, Ï„, num_samples, inv(md_Î»), num_md_updates,
            forward_direction,
        )
    end
end

function ContinuousRKLKL(
    Ï„, md_Î», num_md_updates;
    forward_direction, num_samples, baseline_actions, reparam
)
    ContinuousRKLKL(
        reparam, baseline_actions, Ï„, md_Î», num_md_updates, forward_direction, num_samples,
    )
end

function setup(
    up::ContinuousRKLKL,
    ::AbstractEnvironment,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    rng::AbstractRNG;
)::UpdateState{ContinuousRKLKL}
    return UpdateState(
        up,
        optim,
        (
            optim = Optimisers.setup(optim, Ï€_Î¸),
            rng = Lux.replicate(rng),
            Î¸_t = Ï€_Î¸,    # These are immutable
            state_t = Ï€_st,  # These are immutable
            current_update = 1,
        ),
    )
end

function update(
    st::UpdateState{ContinuousRKLKL},
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,                    # policy model
    Ï€_Î¸,                    # policy model parameters
    Ï€_st,                   # policy model state
    qf::Q,
    qf_f,                   # q function model
    qf_Î¸,                   # q function model parameters
    qf_st,                  # q function model state
    states::AbstractArray,  # Must be >= 2D
)
    up = st._update
    rng = Lux.replicate(st._state.rng)

    # Frozen current policy parameters, must stay fixed during the MD update and only update
    # every up._num_md_updates
    Î¸_t = st._state.Î¸_t
    # State of the current policy, which will change during the MD update
    state_t = st._state.state_t

    âˆ‡Ï€_Î¸, Ï€_st, qf_st, state_t = if !up._reparam
        if up._forward_direction
            _fkl_ll_grad(
                up, rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, states, Î¸_t, state_t,
            )
        else
            _rkl_ll_grad(
                up, rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, states, Î¸_t, state_t,
            )
        end
    else
        if up._forward_direction
            error("cannot use reparameterised gradient estimation with an FKL penalty")
        else
            _rkl_reparam_grad(
                up, rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, states, Î¸_t, state_t,
            )
        end
    end

    Ï€_optim_state = st._state.optim
    Ï€_optim_state, Ï€_Î¸ = Optimisers.update(Ï€_optim_state, Ï€_Î¸, only(âˆ‡Ï€_Î¸))

    next_update = mod(st._state.current_update, up._num_md_updates) + 1
    return UpdateState(
        st._update,
        st._optim,
        (
            optim = Ï€_optim_state,
            rng = rng,
            Î¸_t = next_update == 1 ? Ï€_Î¸ : Î¸_t,
            state_t = next_update == 1 ? Ï€_st : state_t,
            current_update = next_update,
        ),
    ), Ï€_Î¸, Ï€_st, qf_st
end

function _rkl_ll_grad(
    up::ContinuousRKLKL,
    rng::AbstractRNG,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,                            # policy model
    Ï€_Î¸,                            # policy model parameters
    Ï€_st,                           # policy model state
    qf::Q,
    qf_f,                           # q function model
    qf_Î¸,                           # q function model parameters
    qf_st,                          # q function model state
    state_batch::AbstractArray,     # Must be >= 2D
    Î¸_t,                            # Previous policy parameters,
    state_t,                           # Previous policy state
)
    batch_size = size(state_batch)[end]
    state_size = size(state_batch)[begin:end-1]

    # Estimate the approximate state value for each state using v(s) = ğ”¼[q(s, A)]
    if up._baseline_actions > 0
        v, Ï€_st, qf_st = _get_baseline(
            rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, state_batch,
            up._baseline_actions,
        )
    end

    âˆ‡Ï€ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Sample actions for each state in the batch, needed to estimate the gradient in
        # each state: âˆ‡J = ğ”¼[q(S, A) - Ï„*ln(Ï€(Aâˆ£S)]
        actions, lnÏ€_Î¸, Ï€_st = sample_with_logprob(
            Ï€, rng, Ï€_f, Ï€_Î¸, Ï€_st, state_batch; num_samples=up._num_samples,
        )
        action_size = size(actions, 1)
        actions = reshape(actions, :, batch_size)

        # Repeat states for each action sample in each state
        states = repeat(
            state_batch;
            inner = (ones(Int, length(state_size))..., up._num_samples),
        )

        # Calculate the advantage
        q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states, actions; reduct=true)
        q = reshape(q, up._num_samples, batch_size)
        adv = up._baseline_actions > 0 ? q .- v : q

        lnÏ€_Î¸ = reshape(lnÏ€_Î¸, size(adv))
        lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states, actions)
        lnÏ€_t = reshape(lnÏ€_t, size(adv))

        scale = ChainRulesCore.ignore_derivatives(
            -adv .- up._inv_Î» .* lnÏ€_t .+ (up._temperature + up._inv_Î») .* lnÏ€_Î¸
        )

        gpu_mean(lnÏ€_Î¸ .* scale)
    end

    return âˆ‡Ï€, Ï€_st, qf_st, state_t
end

function _rkl_reparam_grad(
    up::ContinuousRKLKL,
    rng::AbstractRNG,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,                            # policy model
    Ï€_Î¸,                            # policy model parameters
    Ï€_st,                           # policy model state
    qf::Q,
    qf_f,                           # q function model
    qf_Î¸,                           # q function model parameters
    qf_st,                          # q function model state
    state_batch::AbstractArray,     # Must be >= 2D
    Î¸_t,                            # Previous policy parameters,
    state_t,                           # Previous policy state
)
    batch_size = size(state_batch)[end]
    state_size = size(state_batch)[begin:end-1]

    # Estimate the approximate state value for each state using v(s) = ğ”¼[q(s, A)]
    if up._baseline_actions > 0
        v, Ï€_st, qf_st = _get_baseline(
            rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, state_batch,
            up._baseline_actions,
        )
    end

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        actions, lnÏ€_Î¸, Ï€_st = rsample_with_logprob(
            Ï€, rng, Ï€_f, Ï€_Î¸, Ï€_st, state_batch; num_samples=up._num_samples,
        )
        action_size = size(actions, 1)
        actions = reshape(actions, action_size, :)

        states = repeat(
            state_batch;
            inner = (ones(Int, length(state_size))..., up._num_samples),
        )

        # Compute the advantage
        q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states, actions)
        q = reshape(q, up._num_samples, batch_size)
        adv = up._baseline_actions > 0 ? q .- v : q

        lnÏ€_Î¸ = reshape(lnÏ€_Î¸, size(adv))
        lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states, actions)
        lnÏ€_t = reshape(lnÏ€_t, size(adv))

        loss = -adv .- up._inv_Î» .* lnÏ€_t .+ (up._temperature + up._inv_Î») .* lnÏ€_Î¸
        return gpu_mean(loss)
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st, state_t
end

function _fkl_ll_grad(
    up::ContinuousRKLKL,
    rng::AbstractRNG,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,                            # policy model
    Ï€_Î¸,                            # policy model parameters
    Ï€_st,                           # policy model state
    qf::Q,
    qf_f,                           # q function model
    qf_Î¸,                           # q function model parameters
    qf_st,                          # q function model state
    state_batch::AbstractArray,     # Must be >= 2D
    Î¸_t,                            # Previous policy parameters,
    state_t,                           # Previous policy state
)
    error("not implemented")
    batch_size = size(state_batch)[end]
    state_size = size(state_batch)[begin:end-1]
    num_samples = up._num_samples

    # Estimate the approximate state value for each state using v(s) = ğ”¼[q(s, A)]
    if up._baseline_actions > 0
        v, Ï€_st, qf_st = _get_baseline(
            rng, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, state_batch,
            up._baseline_actions,
        )
    end

    âˆ‡Ï€ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Sample actions for each state in the batch, needed to estimate the gradient in
        # each state: âˆ‡J = ğ”¼[q(S, A) - Ï„*ln(Ï€(Aâˆ£S)]
        n_entropy_samples = num_samples
        actions, lnÏ€_t, state_t = sample_with_logprob(
            Ï€, rng, Ï€_f, Î¸_t, state_t, state_batch;
            num_samples=(num_samples + n_entropy_samples),
        )
        entropy_Ï€_t = -mean(lnÏ€_t[end-n_entropy_samples+1:end, :]; dims=1)
        lnÏ€_t = lnÏ€_t[begin:begin+num_samples-1, :]

        actions = actions[:, begin:begin+num_samples-1, :]

        action_size = size(actions, 1)
        actions = reshape(actions, :, batch_size)

        # Repeat states for each action sample in each state
        states = repeat(
            state_batch;
            inner = (ones(Int, length(state_size))..., num_samples),
        )

        # Calculate the advantage
        q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states, actions; reduct=true)
        q = reshape(q, num_samples, batch_size)
        adv = up._baseline_actions > 0 ? q .- v : q

        lnÏ€_t = reshape(lnÏ€_t, size(adv))
        lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states, actions)
        lnÏ€_Î¸ = reshape(lnÏ€_Î¸, size(adv))

        Ï‚ = ChainRulesCore.ignore_derivatives(
            adv .- up._temperature .* (lnÏ€_t .+ entropy_Ï€_t)
        )

        -gpu_mean((Ï‚ .+ up._inv_Î») .* lnÏ€_Î¸)
    end

    return âˆ‡Ï€, Ï€_st, qf_st, state_t
end
