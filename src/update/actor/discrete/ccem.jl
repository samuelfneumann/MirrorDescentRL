"""
    DiscreteCCEM

`DiscreteCCEM` implements the discrete-action Conditional Cross-Entropy Method (CCEM) for
policy improvement [1].

# References

[1] S. Neumann, Lim, S., Joseph, A., Pan, Y., White, A., White, M. Greedy
Actor-Critic: A New Conditional Cross-Entropy Method for Policy
Improvement. 2022.
"""
struct DiscreteCCEM <: AbstractActorUpdate
    # Actor Policy Entropy Regularization
    _Ï„::Float32         # Actor Temperature

    function DiscreteCCEM(Ï„::Real)
        @assert (Ï„ >= 0) "expected Ï„ >= 0"
        return new(Ï„)
    end
end

function setup(
    up::DiscreteCCEM,
    ::AbstractEnvironment,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    ::AbstractRNG,
)::UpdateState{DiscreteCCEM}
    return UpdateState(
        up,
        optim,
        (Ï€_optim = Optimisers.setup(optim, Ï€_Î¸),)
    )
end

function setup(
    up::DiscreteCCEM,
    ::AbstractEnvironment,
    Ï€::SimplexPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Union{Nothing,Optimisers.AbstractRule},
    _::AbstractRNG,
)
    error("cannot use DiscreteCCEM with simplex policies")
end

function update(
    st::UpdateState{DiscreteCCEM},
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    up = st._update
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    batch_size = size(states)[end]

    âˆ‡Ï€_Î¸, Ï€_st, qf_st = _gradient(up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, states)

    Ï€_optim_state = st._state.Ï€_optim
    Ï€_optim_state, Ï€_Î¸ = Optimisers.update(Ï€_optim_state, Ï€_Î¸, only(âˆ‡Ï€_Î¸))

    return UpdateState(
        st._update,
        st._optim,
        (Ï€_optim = Ï€_optim_state,),
    ), Ï€_Î¸, Ï€_st, qf_st
end

function _gradient(
    up::DiscreteCCEM,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    batch_size = size(states)[end]

    # Find the indices of the actions of maximal value
    ind = mapslices(x -> sortperm(x; rev=true), q; dims=1)
    ind = ind[1, :]
    top_ind = CartesianIndex.(ind, LinearIndices(ind))

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Compute the gradient âˆ‡J = ð”¼_{I*}[ln(Ï€)]
        lnÏ€, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)
        if up._Ï„ > 0
            # Compute entropy regularization
            H, Ï€_st = entropy(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)
            -gpu_mean(lnÏ€[top_ind]) - up._Ï„ * gpu_mean(H)
        else
            -gpu_mean(lnÏ€[top_ind])
        end
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st
end
