"""
    DiscreteMPO

DiscreteMPO implements an MPO-like update, without a KL penalty
"""
struct DiscreteMPO <: AbstractActorUpdate
    _kl_policy_coeff::Float32
    _Ï„::Float32

    _use_baseline::Bool
    _Îµ::Float32

    function DiscreteMPO(kl_policy_coeff::Real, Ï„::Real, use_baseline, Îµ::Real=0f0)
        @assert (kl_policy_coeff > 0) "expected kl_policy_coeff > 0"
        @assert (Ï„ >= 0) "expected Ï„ >= 0"
        @assert (Îµ >= 0) "expected Îµ >= 0"

        return new(kl_policy_coeff, Ï„, use_baseline, Îµ)
    end
end

function DiscreteMPO(kl_policy_coeff::Real, Ï„::Real; use_baseline, Îµ::Real=0f0)
    return DiscreteMPO(kl_policy_coeff, Ï„, use_baseline, Îµ)
end

function setup(
    up::DiscreteMPO,
    ::AbstractEnvironment,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    ::AbstractRNG,
)::UpdateState{DiscreteMPO}
    return UpdateState(
        up,
        optim,
        (Ï€_optim = Optimisers.setup(optim, Ï€_Î¸),)
    )
end

function setup(
    up::DiscreteMPO,
    Ï€::SimplexPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Union{Nothing,Optimisers.AbstractRule},
    _::AbstractRNG,
)
    error("cannot use DiscreteMPO with simplex policies")
end

function update(
    st::UpdateState{DiscreteMPO},
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    up = st._update

    âˆ‡Ï€_Î¸, Ï€_st, qf_st = _gradient(
        up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, states,
    )

    Ï€_optim_state = st._state.Ï€_optim
    Ï€_optim_state, Ï€_Î¸ = Optimisers.update(Ï€_optim_state, Ï€_Î¸, only(âˆ‡Ï€_Î¸))

    return UpdateState(
        st._update,
        st._optim,
        (Ï€_optim = Ï€_optim_state,),
    ), Ï€_Î¸, Ï€_st, qf_st
end

function _gradient(
    up::DiscreteMPO,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    adv = up._use_baseline ? q .- mean(q; dims=1) : q

    # Calculate the KL policy in Ï€
    scale = adv ./ up._kl_policy_coeff
    scale = exp.(scale .- maximum(scale; dims=1))

    batch_size = size(states)[end]

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Compute the gradient âˆ‡J = ð”¼_{I*}[ln(Ï€)]
        lnÏ€, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)

        # Because we are in the discrete-action setting, we can directly compute Ï€_{KL} and
        # use it rather than using values that are âˆ Ï€_{KL}(a âˆ£ s)
        probs = ChainRulesCore.ignore_derivatives(exp.(lnÏ€))
        Ï€_KL_numerator = scale .* probs .+ up._Îµ
        Ï€_KL = ChainRulesCore.ignore_derivatives(
            Ï€_KL_numerator ./ sum(Ï€_KL_numerator; dims=1)
        )

        if up._Ï„ > 0
            # Compute entropy regularization
            H, Ï€_st = entropy(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)
            -gpu_mean(sum(Ï€_KL .* lnÏ€; dims=1)) - up._Ï„ * gpu_mean(H)
        else
            -gpu_mean(sum(Ï€_KL .* lnÏ€; dims=1))
        end
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st
end

function _gradient(
    up::DiscreteMPO,
    Ï€::SoftmaxPolicy,
    Ï€_f::Tabular,   # actor policy model
    Ï€_Î¸,            # actor policy model parameters
    Ï€_st,           # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractMatrix{Int},
)
    # Compute advantages
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st)
    adv = up._use_baseline ? q .- mean(q; dims=1) : q
    @assert ndims(adv) == 2

    # Compute KL policy in Ï€â‚œ
    probs, Ï€_st = prob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st)
    Ï€_KL_logits = adv ./ up._kl_policy_coeff
    Ï€_KL_logits_exp = exp.(Ï€_KL_logits .- maximum(Ï€_KL_logits; dims=1))
    Ï€_KL_numerator = Ï€_KL_logits_exp .* probs .+ up._Îµ
    Ï€_KL = Ï€_KL_numerator ./ sum(Ï€_KL_numerator; dims=1)

    ####################################################################
    # Manual gradient calculation
    ####################################################################
    gs = zero(Float32, Ï€_f)
    batch_size = size(states, 2)
    gs = treemap!(gs) do g_i
        for i in 1:batch_size
            s_t = states[1, i]
            g = -Ï€_KL[:, s_t]' .* _âˆ‡ln_softmax_tabular(
                Ï€_Î¸.layer_1, s_t; sum_over_actions=false,
            )
            g = sum(g; dims=2)

            if up._Ï„ > zero(up._Ï„)
                g .-= (up._Ï„ .* _âˆ‡entropy_softmax_tabular(Ï€_Î¸.layer_1, s_t))
            end

            g_i[:, s_t] .+= (g ./ batch_size)
        end
        g_i
    end

    return (gs,), Ï€_st, qf_st
end
