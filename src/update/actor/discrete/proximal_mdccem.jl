"""
    DiscreteProximalMDCCEM

`DiscreteProximalMDCCEM` uses a functional proximal mirror descent CCEM update with discrete
actions. Crucially, the policy parameterization must ensure valid policies (i.e.
distributions must sum to 1). This ensures that no projection operator is needed to project
distributions back to a valid distribution space. See `SimplexProximalMDCCEM` for an
implementation that works on the simplex, which needs a projection operation after making
policy updates.

The functional mirror descent update is applied on the policy distribution itself,
using a negative entropy mirror map.

When using softmax policies, the functional mirror descent update can be applied either on
the policy distribution itself or on the softmax logits using a negative entropy mirror map
or a log-sum-exp mirror map respectively.

Uses Mini-Batch style updates.
"""
struct DiscreteProximalMDCCEM <: AbstractActorUpdate
    # Actor Policy Entropy Regularization
    _Ï„::Float32         # Actor Temperature

    _inv_Î»::Float32     # Inverse stepsize for mirror descent (functional) update
    _num_md_updates::Int

    _forward_direction::Bool

    function DiscreteProximalMDCCEM(
        Ï„::Real,  md_Î»::Real, num_md_updates::Int, forward_direction::Bool,
    )
        @assert (num_md_updates > 1) "expected num_md_updates > 1"
        @assert (md_Î» > 0f0) "expected functional stepsize md_Î» > 0)"
        @assert (Ï„ >= 0) "expected Ï„ >= 0"

        return new(Ï„, inv(md_Î»), num_md_updates, forward_direction)
    end
end

function DiscreteProximalMDCCEM(
    Ï„::Real,  md_Î»::AbstractFloat, num_md_updates::Int; forward_direction::Bool,
)
    DiscreteProximalMDCCEM(Ï„, md_Î», num_md_updates, forward_direction)
end

function setup(
    up::DiscreteProximalMDCCEM,
    env::AbstractEnvironment,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    rng::AbstractRNG,
)::UpdateState{DiscreteProximalMDCCEM}
    # Setup gradient cache
    return UpdateState(
        up,
        optim,
        (
            optim = Optimisers.setup(optim, Ï€_Î¸),
            # Previous policy parameters for the KL update
            Î¸_t = Ï€_Î¸,    # These are immutable
            state_t = Ï€_st,  # These are immutable
            current_update = 1,
        )
    )
end

function setup(
    up::DiscreteProximalMDCCEM,
    env::AbstractEnvironment,
    Ï€::SimplexPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    rng::AbstractRNG,
)::UpdateState{DiscreteProximalMDCCEM}
    error("cannot use SimplexPolicy with DiscreteProximalMDCCEM")
end

function update(
    st::UpdateState{DiscreteProximalMDCCEM},
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    up = st._update

    # Frozen current policy parameters, must stay fixed during the MD update and only update
    # every up._num_md_updates
    Î¸_t = st._state.Î¸_t
    # State of the current policy, which will change during the MD update
    state_t = st._state.state_t

    âˆ‡Ï€, Ï€_st, qf_st, st_t = if !up._forward_direction
        _rkl_gradient(
            up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Î¸_t, state_t, qf, qf_f, qf_Î¸, qf_st, states,
        )
    else
        _fkl_gradient(
            up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Î¸_t, state_t, qf, qf_f, qf_Î¸, qf_st, states,
        )
    end

    optim_state = st._state.optim
    optim_state, Ï€_Î¸ = Optimisers.update(optim_state, Ï€_Î¸, only(âˆ‡Ï€))

    next_update = mod(st._state.current_update, up._num_md_updates) + 1
    return UpdateState(
        st._update,
        st._optim,
        (
            optim = optim_state,
            Î¸_t = next_update == 1 ? Ï€_Î¸ : Î¸_t,
            state_t = next_update == 1 ? Ï€_st : state_t,
            current_update = next_update,
        ),
    ), Ï€_Î¸, Ï€_st, qf_st
end

function _rkl_gradient(
    up::DiscreteProximalMDCCEM,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    batch_size = size(states)[end]

    # Find the indices of the actions of maximal value
    ind = mapslices(x -> sortperm(x; rev=true), q; dims=1)
    top_ind = [CartesianIndex(ind[1, j], j) for j in 1:batch_size]

    lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states)
    best_lnÏ€_t = exp.(lnÏ€_t[top_ind])

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Compute the gradient âˆ‡J = ğ”¼_{I*}[ln(Ï€)]
        lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)
        Ï€_Î¸ = exp.(lnÏ€_Î¸)
        best_lnÏ€_Î¸ = lnÏ€_Î¸[top_ind]

        lr_term = exp.(best_lnÏ€_Î¸ .- best_lnÏ€_t)
        kl_entropy_term = ChainRulesCore.ignore_derivatives(
            (up._Ï„ - up._inv_Î») .* lnÏ€_t .+ up._inv_Î» .* lnÏ€_Î¸
        ) .* Ï€_Î¸
        kl_entropy_term = dropdims(sum(kl_entropy_term; dims=1); dims=1)

        loss = -(lr_term .- kl_entropy_term)
        gpu_mean(loss)
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st, state_t
end

function _fkl_gradient(
    up::DiscreteProximalMDCCEM,
    Ï€::SoftmaxPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    batch_size = size(states)[end]

    # Find the indices of the actions of maximal value
    ind = mapslices(x -> sortperm(x; rev=true), q; dims=1)
    top_ind = [CartesianIndex(ind[1, j], j) for j in 1:batch_size]

    lnÏ€_t, st_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states)
    Ï€_t = exp.(lnÏ€_t)
    @tullio entropy_t[i] := -Ï€_t[j, i] * lnÏ€_t[j, i]

    Ï‚ = 1 .+ up._Ï„ .* (lnÏ€_t .+ reshape(entropy_t, 1, :))

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Compute the gradient âˆ‡J = ğ”¼_{I*}[ln(Ï€)]
        lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)
        best_lnÏ€_Î¸ = lnÏ€_Î¸[top_ind]

        loss = -(best_lnÏ€_Î¸ .- sum(Ï€_t .* (Ï‚ .- up._inv_Î») .* lnÏ€_Î¸; dims=1))
        gpu_mean(loss)
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st, st_t
end

function _rkl_gradient(
    up::DiscreteProximalMDCCEM,
    Ï€::SoftmaxPolicy,
    Ï€_f::Tabular,   # actor policy model
    Ï€_Î¸,            # actor policy model parameters
    Ï€_st,           # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray{Int}, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    batch_size = size(states)[end]

    # Find the indices of the actions of maximal value
    ind = mapslices(x -> sortperm(x; rev=true), q; dims=1)
    top_actions = [ind[1, j] for j in 1:batch_size]

    ####################################################################
    # Manual gradient calculation
    ####################################################################
    gs = zero(Float32, Ï€_f)
    lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t)
    lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st)
    treemap!(gs) do g_i
        for i in 1:batch_size
            s_t = states[1, i]
            a_t = top_actions[i]

            lr_term = exp.(_âˆ‡ln_softmax_tabular(Ï€_Î¸.layer_1, s_t, a_t) .- lnÏ€_t[a_t, s_t])

            âˆ‡Ï€_Î¸ = _âˆ‡_softmax_tabular(Ï€_Î¸.layer_1, s_t; sum_over_actions=false)
            kl_entropy_term = (
                (up._Ï„ - up._inv_Î») .* lnÏ€_t[:, s_t] +
                up._inv_Î» .* lnÏ€_Î¸[:, s_t]
            )
            kl_entropy_term = reshape(kl_entropy_term, 1, :)
            âˆ‡Ï€_Î¸ .*= kl_entropy_term
            âˆ‡Ï€_Î¸_term = dropdims(sum(âˆ‡Ï€_Î¸; dims=2); dims=2)

            g_i[:, s_t] .-= ((lr_term .- âˆ‡Ï€_Î¸_term) ./ batch_size)
            end
        g_i
    end


    return (gs,), Ï€_st, qf_st, state_t
end

function _fkl_gradient(
    up::DiscreteProximalMDCCEM,
    Ï€::SoftmaxPolicy,
    Ï€_f::Tabular,   # actor policy model
    Ï€_Î¸,            # actor policy model parameters
    Ï€_st,           # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray{Int}, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    batch_size = size(states)[end]

    # Find the indices of the actions of maximal value
    ind = mapslices(x -> sortperm(x; rev=true), q; dims=1)
    top_actions = [ind[1, j] for j in 1:batch_size]

    ####################################################################
    # Manual gradient calculation
    ####################################################################
    gs = spzeros(Float32, Ï€_f)
    lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t)
    Ï€_t = exp.(lnÏ€_t)

    entropy_t = sum(Ï€_t .* lnÏ€_t; dims=1)

    Ï‚ = 1 .+ up._Ï„ .* (lnÏ€_t .+ entropy_t)

    treemap!(gs) do g_i
        for i in 1:batch_size
            s_t = states[1, i]
            a_t = top_actions[i]

            # Compute expectation argument
            lr_term = _âˆ‡ln_softmax_tabular(Ï€_Î¸.layer_1, s_t; sum_over_actions=false)

            Ï‚_s_t = Ï‚[:, s_t:s_t]'
            expectation_arg = (Ï‚_s_t .- up._inv_Î») .* lr_term

            # Compute expectation ğ”¼_{Ï€â‚œ} [(Ï‚(aâ‚œ, sâ‚œ) + 1/Î») ln(Ï€(a | s, Î¸))]
            Ï€_t_s_t = Ï€_t[:, s_t:s_t]'
            expectation = sum(Ï€_t_s_t .* expectation_arg; dims=2)
            expectation = dropdims(expectation; dims=2)

            g_i[:, s_t] .-= (
                _âˆ‡ln_softmax_tabular(Ï€_Î¸.layer_1, s_t, a_t) .- expectation
            ) ./ batch_size
            end
        g_i
    end

    return (gs,), Ï€_st, qf_st, state_t
end

