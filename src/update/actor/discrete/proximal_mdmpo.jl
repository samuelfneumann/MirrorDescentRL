"""
    DiscreteProximalMDMPO(n, Ï, ÏÌƒ)

`DiscreteProximalMDMPO` uses a functional proximal mirror descent MPO update with discrete
actions. Crucially, the policy parameterization must ensure valid policies (i.e.
distributions must sum to 1). This ensures that no projection operator is needed to project
distributions back to a valid distribution space. See `SimplexProximalMDMPO` for an
implementation that works on the simplex, which needs a projection operation after making
policy updates.

The functional mirror descent update is applied on the policy distribution itself,
using a negative entropy mirror map.

When using softmax policies, the functional mirror descent update can be applied either on
the policy distribution itself or on the softmax logits using a negative entropy mirror map
or a log-sum-exp mirror map respectively.

Uses Mini-Batch style updates.
"""
struct DiscreteProximalMDMPO <: AbstractActorUpdate
    # Actor Policy Entropy Regularization
    _Ï„::Float32         # Actor Temperature
    _kl_policy_coeff::Float32

    _inv_Î»::Float32     # Inverse stepsize for mirror descent (functional) update
    _num_md_updates::Int

    _forward_direction::Bool
    _use_baseline::Bool

    function DiscreteProximalMDMPO(
        Ï„::Real, kl_policy_coeff::Real, md_Î»::AbstractFloat, num_md_updates::Int,
        use_baseline::Bool, forward_direction::Bool,
    )
        @assert (num_md_updates > 1) "expected num_md_updates > 1"
        @assert (md_Î» > 0f0) "expected functional stepsize md_Î» > 0)"
        @assert (Ï„ >= 0) "expected Ï„ >= 0"
        @assert (kl_policy_coeff > 0) "expected kl_policy_coeff >= 0"

        return new(
            Ï„, kl_policy_coeff, inv(md_Î»), num_md_updates, forward_direction, use_baseline,
        )
    end
end

function DiscreteProximalMDMPO(
    Ï„::Real, kl_policy_coeff::Real, md_Î»::AbstractFloat, num_md_updates::Int;
    use_baseline::Bool, forward_direction::Bool,
)
    return DiscreteProximalMDMPO(
        Ï„, kl_policy_coeff, md_Î», num_md_updates, use_baseline, forward_direction,
    )
end

function setup(
    up::DiscreteProximalMDMPO,
    env::AbstractEnvironment,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractActionValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    optim::Optimisers.AbstractRule,
    rng::AbstractRNG,
)::UpdateState{DiscreteProximalMDMPO}
    # Setup gradient cache
    return UpdateState(
        up,
        optim,
        (
            optim = Optimisers.setup(optim, Ï€_Î¸),
            # Previous policy parameters for the KL update
            Î¸_t = Ï€_Î¸,    # These are immutable
            state_t = Ï€_st,  # These are immutable
            current_update = 1,
        )
    )
end

function update(
    st::UpdateState{DiscreteProximalMDMPO},
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    up = st._update

    # Frozen current policy parameters, must stay fixed during the MD update and only update
    # every up._num_md_updates
    Î¸_t = st._state.Î¸_t
    # State of the current policy, which will change during the MD update
    state_t = st._state.state_t

    âˆ‡Ï€, Ï€_st, qf_st, st_t = if !up._forward_direction
        _rkl_gradient(
            up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Î¸_t, state_t, qf, qf_f, qf_Î¸, qf_st, states,
        )
    else
        _fkl_gradient(
            up, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Î¸_t, state_t, qf, qf_f, qf_Î¸, qf_st, states,
        )
    end

    optim_state = st._state.optim
    optim_state, Ï€_Î¸ = Optimisers.update(optim_state, Ï€_Î¸, only(âˆ‡Ï€))

    next_update = mod(st._state.current_update, up._num_md_updates) + 1
    return UpdateState(
        st._update,
        st._optim,
        (
            optim = optim_state,
            Î¸_t = next_update == 1 ? Ï€_Î¸ : Î¸_t,
            state_t = next_update == 1 ? Ï€_st : state_t,
            current_update = next_update,
        ),
    ), Ï€_Î¸, Ï€_st, qf_st
end

function _rkl_gradient(
    up::DiscreteProximalMDMPO,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # actor policy model
    Ï€_Î¸,    # actor policy model parameters
    Ï€_st,   # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray, # Must be >= 2D
)
    # Compute advantages
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    adv = up._use_baseline ? q .- mean(q; dims=1) : q
    @assert ndims(adv) == 2

    lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states)
    Ï€_t = exp.(lnÏ€_t)

    # Compute KL policy in Ï€â‚œ
    Ï€_KL_logits = adv ./ up._kl_policy_coeff
    Ï€_KL_logits .-= maximum(Ï€_KL_logits; dims=1)
    Ï€_KL_logits_exp = exp.(Ï€_KL_logits)
    Ï€_KL_numerator = Ï€_KL_logits_exp .* Ï€_t
    # Ï€_KL = Ï€_KL_numerator ./ sum(Ï€_KL_numerator; dims=1)
    lnÏ€_KL = Ï€_KL_logits .+ lnÏ€_t .- log.(sum(Ï€_KL_numerator; dims=1))

    scale1 = exp.(lnÏ€_KL .- lnÏ€_t) .- ((up._Ï„ - up._inv_Î») .* lnÏ€_t)

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        # Compute the gradient âˆ‡J = ğ”¼_{I*}[ln(Ï€)]
        lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)
        Ï€_Î¸ = exp.(lnÏ€_Î¸)

        scale2 = up._inv_Î» .* ChainRulesCore.ignore_derivatives(lnÏ€_Î¸)

        loss = -sum((scale1 - scale2) .* Ï€_Î¸; dims=1)
        gpu_mean(loss)
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st, state_t
end

 function _fkl_gradient(
     up::DiscreteProximalMDMPO,
     Ï€::SoftmaxPolicy,
     Ï€_f,    # actor policy model
     Ï€_Î¸,    # actor policy model parameters
     Ï€_st,   # actor policy model state
     Î¸_t,
     state_t,
     qf::DiscreteQ,
     qf_f,
     qf_Î¸,
     qf_st,
     states::AbstractArray, # Must be >= 2D
 )
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    adv = up._use_baseline ? q .- mean(q; dims=1) : q
    batch_size = size(states)[end]

    lnÏ€_t, st_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states)
    Ï€_t = exp.(lnÏ€_t)
    @tullio entropy_t[i] := -Ï€_t[j, i] * lnÏ€_t[j, i]

    # Compute KL policy in Ï€â‚œ
    Ï€_KL_logits = adv ./ up._kl_policy_coeff
    Ï€_KL_logits .-= maximum(Ï€_KL_logits; dims=1)
    Ï€_KL_logits_exp = exp.(Ï€_KL_logits)
    Ï€_KL_numerator = Ï€_KL_logits_exp .* Ï€_t
    Ï€_KL = Ï€_KL_numerator ./ sum(Ï€_KL_numerator; dims=1)

    Ï‚ = 1 .+ up._Ï„ .* (lnÏ€_t .+ reshape(entropy_t, 1, :))

    âˆ‡Ï€_Î¸ = gradient(Ï€_Î¸) do Ï€_Î¸
        lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)

        term1 = sum(Ï€_KL .* lnÏ€_Î¸; dims=1)

        expectation_arg = (Ï‚ .- up._inv_Î») .* lnÏ€_Î¸
        term2 = sum(Ï€_t .* expectation_arg; dims=1)

        loss = term2 - term1
        gpu_mean(loss)
    end

    return âˆ‡Ï€_Î¸, Ï€_st, qf_st, st_t
end

function _rkl_gradient(
    up::DiscreteProximalMDMPO,
    Ï€::SoftmaxPolicy,
    Ï€_f::Tabular,   # actor policy model
    Ï€_Î¸,            # actor policy model parameters
    Ï€_st,           # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray{Int}, # Must be >= 2D
)
    # Compute advantages
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    adv = up._use_baseline ? q .- mean(q; dims=1) : q
    @assert ndims(adv) == 2
    batch_size = size(adv, 2)

    lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states)
    Ï€_t = exp.(lnÏ€_t)

    # Compute KL policy in Ï€â‚œ
    Ï€_KL_logits = adv ./ up._kl_policy_coeff
    Ï€_KL_logits .-= maximum(Ï€_KL_logits; dims=1)
    Ï€_KL_logits_exp = exp.(Ï€_KL_logits)
    Ï€_KL_numerator = Ï€_KL_logits_exp .* Ï€_t
    # Ï€_KL = Ï€_KL_numerator ./ sum(Ï€_KL_numerator; dims=1)
    lnÏ€_KL = Ï€_KL_logits .+ lnÏ€_t .- log.(sum(Ï€_KL_numerator))

    ####################################################################
    # Manual gradient calculation
    ####################################################################
    lnÏ€_Î¸, Ï€_st = logprob(Ï€, Ï€_f, Ï€_Î¸, Ï€_st)
    gs = zero(Float32, Ï€_f)
    treemap!(gs) do g_i
        for i in 1:batch_size
            s_t = states[1, i]

            lr_term = exp.(lnÏ€_KL .- lnÏ€_t)
            entropy_t_term = (up._Ï„ - up._inv_Î») .* lnÏ€_t[:, i]
            entropy_term = up._inv_Î» .* lnÏ€_Î¸[:, s_t]

            scale = lr_term .- entropy_t_term .- entropy_term

            âˆ‡Ï€_Î¸ = _âˆ‡_softmax_tabular(Ï€_Î¸.layer_1, s_t; sum_over_actions=false)
            âˆ‡Ï€_Î¸ .*= reshape(scale, 1, :)
            âˆ‡Ï€_Î¸_term = dropdims(sum(âˆ‡Ï€_Î¸; dims=2); dims=2)

            g_i[:, s_t] .-= (âˆ‡Ï€_Î¸_term ./ batch_size)
            end
        g_i
    end

    return (gs,), Ï€_st, qf_st, state_t
end

function _fkl_gradient(
    up::DiscreteProximalMDMPO,
    Ï€::SoftmaxPolicy,
    Ï€_f::Tabular,   # actor policy model
    Ï€_Î¸,            # actor policy model parameters
    Ï€_st,           # actor policy model state
    Î¸_t,
    state_t,
    qf::DiscreteQ,
    qf_f,
    qf_Î¸,
    qf_st,
    states::AbstractArray{Int}, # Must be >= 2D
)
    q, qf_st = predict(qf, qf_f, qf_Î¸, qf_st, states)
    adv = up._use_baseline ? q .- mean(q; dims=1) : q
    @assert ndims(adv) == 2
    batch_size = size(adv, 2)

    lnÏ€_t, state_t = logprob(Ï€, Ï€_f, Î¸_t, state_t, states)
    Ï€_t = exp.(lnÏ€_t)

    entropy_t = sum(Ï€_t .* lnÏ€_t; dims=1)
    Ï‚ = 1 .+ up._Ï„ .* (lnÏ€_t .+ entropy_t)

    # Compute KL policy in Ï€â‚œ
    Ï€_KL_logits = adv ./ up._kl_policy_coeff
    Ï€_KL_logits .-= maximum(Ï€_KL_logits; dims=1)
    Ï€_KL_logits_exp = exp.(Ï€_KL_logits)
    Ï€_KL_numerator = Ï€_KL_logits_exp .* Ï€_t
    Ï€_KL = Ï€_KL_numerator ./ sum(Ï€_KL_numerator; dims=1)

    ####################################################################
    # Manual gradient calculation
    ####################################################################
    gs = spzeros(Float32, Ï€_f)
    treemap!(gs) do g_i
        for i in 1:batch_size
            s_t = states[1, i]

            âˆ‡lnÏ€_Î¸ = _âˆ‡ln_softmax_tabular(Ï€_Î¸.layer_1, s_t; sum_over_actions=false)

            term1 = dropdims(sum(Ï€_KL[:, i]' .* âˆ‡lnÏ€_Î¸; dims=2); dims=2)

            expectation_arg = (Ï‚[:, i] .- up._inv_Î»)' .* âˆ‡lnÏ€_Î¸
            term2 = dropdims(sum(Ï€_t[:, i]' .* expectation_arg; dims=2); dims=2)

            g_i[:, s_t] .-= ((term1 .- term2) ./ batch_size)
            end
        g_i
    end

    return (gs,), Ï€_st, qf_st, state_t
end
