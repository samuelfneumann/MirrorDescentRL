"""
    AbstractBellmanRegulariser <: AbstractRegulariser

AbstractBellmanRegulariser implements a struct which returns additional terms to add to the
Bellman operator. The Bellman operator ð•‹ is:

  ð•‹q(s, a) = r + Î³q(s', a')

a regulariser adds terms Î¾ which alters the Bellman operator to a regularised Bellman
operator ð’¯:

  ð’¯q(s, a) = r + Î³ (q(s', a') + Î¾)

# References
[1] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, RÃ©mi Munos, Matthieu
Geist. Leverage the Average: an Analysis of KL Regularization in RL. Neurips, 2020.

[2] Matthieu Geist, Bruno Scherrer, Olivier Pietquin. A Theory of Regularized Markov
Decision Processes. ICML, 2019.

"""
abstract type AbstractBellmanRegulariser <: AbstractRegulariser end

function regularise end

function regularise(
    reg_state::RegulariserState{<:AbstractBellmanRegulariser},
    states,
    Ï€::AbstractParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)
    error("regularise not implemented for $(typeof(reg))")
end

function setup(reg::AbstractBellmanRegulariser, args...; kwargs...)
    error(
        "setup not implemented for type $(typeof(reg)) with args $(typeof(args)) " *
        "and kwargs $(typeof(kwargs))"
    )
end

function setup(
    reg::AbstractBellmanRegulariser,
    Ï€::AbstractParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::AbstractValueFunction,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    seed::Integer,
)::RegulariserState{<:AbstractBellmanRegulariser}
    rng = Random.default_rng()
    Random.seed!(rng, seed)
    return setup(reg, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st, rng)
end

####################################################################
# KL Regulariser
#
# The KL regulariser refreshes the previous policy parameters in a
# particular way. In order to properly interleave the parameter
# refreshes for proximal mirror descent actor updates and critic
# updates, the actor should be updated first, followed by the critic.
####################################################################
struct KLBellmanRegulariser <: AbstractBellmanRegulariser
    _inv_Î»::Float32
    _num_md_updates::Int

    # The number of samples to use for estimating the KL divergence. If an analytic form of
    # the KL divergence exists, that is used and this field is ignored
    _num_samples::Int
    _forward_direction::Bool

    function KLBellmanRegulariser(Î», num_md_updates, num_samples, forward_direction)
        error("KLBellmanRegulariser does not work with the new Proximal update interface")

        @assert Î» > 0
        @assert num_md_updates >= 1
        @assert num_samples > 0

        return new(inv(Î»), num_md_updates, num_samples, forward_direction)
    end
end

function KLBellmanRegulariser(Î», num_md_updates, forward_direction)
    KLBellmanRegulariser(Î», num_md_updates, 1, forward_direction)
end

function setup(
    reg::KLBellmanRegulariser,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    rng::AbstractRNG,
)::RegulariserState{KLBellmanRegulariser}
    return RegulariserState(
        reg,
        (
            rng = Lux.replicate(rng),
            Ï†_t = Ï€_Î¸,      # Policy parameters at the previous step
            st_t = Ï€_st,    # Policy function approximator state at the previous step
            current_update = 1,
        ),
    )
end

function setup(
    reg::KLBellmanRegulariser,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::DiscreteQ,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    rng::AbstractRNG,
)::RegulariserState{KLBellmanRegulariser}
    msg = (
        "num_samples should not be modified -- KL is computed exactly in the discrete " *
        "action case"
    )
    @assert reg._num_samples == 1 msg
    return RegulariserState(
        reg,
        (
            Ï†_t = Ï€_Î¸,      # Policy parameters at the previous step
            st_t = Ï€_st,    # Policy function approximator state at the previous step
            current_update = 1,
        ),
    )
end

function regularise(
    state::RegulariserState{KLBellmanRegulariser},
    states,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)

    reg = state._reg
    rng = Lux.replicate(state._state.rng)

    current_update = state._state.current_update
    Ï†_t, st_t = if current_update == 1
        Ï€_Î¸, Ï€_st
    else
        state._state.Ï†_t, state._state.st_t
    end

    kl, Ï€_st, _ = if reg._forward_direction
        kldivergence(
            Ï€, Ï€_f, Ï†_t, st_t, Ï€_Î¸, Ï€_st, states; rng=rng, num_samples=reg._num_samples,
        )
    else
        kldivergence(
            Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Ï†_t, st_t, states; rng=rng, num_samples=reg._num_samples,
        )
    end

    out = -(reg._inv_Î» .* kl)

    next_update = mod(state._state.current_update, reg._num_md_updates) + 1
    return out, Ï€_st, qf_st, RegulariserState(
        reg,
        (
            rng = rng,
            Ï†_t = Ï†_t,
            st_t = st_t,
            current_update = next_update,
        )
    )
end

function regularise(
    state::RegulariserState{KLBellmanRegulariser},
    states,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    _actions::AbstractMatrix,            # (action dims, batch_size)
    _actions_log_prob::AbstractVector,   # (batch_size,)
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)
    return regularise(state, states, Ï€, Ï€_f, Ï€_Î¸, Ï€_st, qf, qf_f, qf_Î¸, qf_st)
end

function regularise(
    state::RegulariserState{KLBellmanRegulariser},
    states,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::DiscreteQ,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)
    reg = state._reg

    current_update = state._state.current_update
    Ï†_t, st_t = if current_update == 1
        Ï€_Î¸, Ï€_st
    else
        state._state.Ï†_t, state._state.st_t
    end

    kl, Ï€_st, _ = if reg._forward_direction
        kldivergence(Ï€, Ï€_f, Ï†_t, st_t, Ï€_Î¸, Ï€_st, states)
    else
        kldivergence(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, Ï†_t, st_t, states)
    end

    out = -(reg._inv_Î» .* kl)
    next_update = mod(state._state.current_update, reg._num_md_updates) + 1
    return out, Ï€_st, qf_st, RegulariserState(
        reg,
        (
            Ï†_t = next_update == 1 ? Ï€_Î¸ : Ï†_t,
            st_t = next_update == 1 ? Ï€_st : st_t,
            current_update = next_update,
        ),
    )
end
####################################################################

####################################################################
# Entropy Regularizer
####################################################################
struct EntropyBellmanRegulariser <: AbstractBellmanRegulariser
    _Ï„::Float32

    # Not used for discrete actions or when providing the regulariser with actions to use
    # for regularisation
    _num_samples::Int

    function EntropyBellmanRegulariser(Ï„, n=1)
        @assert Ï„ > 0
        return new(Ï„, n)
    end
end

function setup(
    reg::EntropyBellmanRegulariser,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    rng::AbstractRNG,
)::RegulariserState{EntropyBellmanRegulariser}
    return RegulariserState(reg, (rng = Lux.replicate(rng),))
end

function setup(
    reg::EntropyBellmanRegulariser,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::DiscreteQ,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
    rng::AbstractRNG,
)::RegulariserState{EntropyBellmanRegulariser}
    msg = (
        "num_samples should not be modified -- entropy is computed exactly in the " *
        "discrete action case"
    )
    @assert reg._num_samples == 1 msg
    return RegulariserState(reg, NamedTuple())
end

function regularise(
    state::RegulariserState{EntropyBellmanRegulariser},
    states,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)
    reg = state._reg
    rng = Lux.replicate(state._state.rng)
    actions, lnÏ€, Ï€_st = sample_with_logprob(
        Ï€, rng, Ï€_f, Ï€_Î¸, Ï€_st, states; num_samples=reg._num_samples,
    )
    entropy = -dropdims(mean(lnÏ€; dims=1); dims=1)
    return (reg._Ï„ * entropy), Ï€_st, qf_st, RegulariserState(reg, (rng = rng,))
end

function regularise(
    state::RegulariserState{EntropyBellmanRegulariser},
    states,
    Ï€::AbstractContinuousParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    actions::AbstractMatrix,            # (action dims, batch_size)
    actions_log_prob::AbstractVector,   # (batch_size,)
    qf::Q,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)
    # Warning: no checks are done to ensure that the actions_log_prob are the actual log
    # probabilities of actions from policy Ï€
    reg = state._reg
    entropy = -actions_log_prob
    rng = Lux.replicate(state._state.rng)
    return (reg._Ï„ * entropy), Ï€_st, qf_st, RegulariserState(reg, (rng = rng,))
end

function regularise(
    state::RegulariserState{EntropyBellmanRegulariser},
    states,
    Ï€::AbstractDiscreteParameterisedPolicy,
    Ï€_f,    # policy model
    Ï€_Î¸,    # policy model parameters
    Ï€_st,   # policy model state
    qf::DiscreteQ,
    qf_f,   # q function model
    qf_Î¸,   # q function model parameters
    qf_st,  # q function model state
)
    reg = state._reg
    H, model_st = entropy(Ï€, Ï€_f, Ï€_Î¸, Ï€_st, states)

    return (reg._Ï„ .* H), Ï€_st, qf_st, RegulariserState(reg, NamedTuple())
end
####################################################################

####################################################################
# Null Regularizer
####################################################################
struct NullBellmanRegulariser <: AbstractBellmanRegulariser end

function setup(
    reg::NullBellmanRegulariser, args...; kwargs...,
)::RegulariserState{NullBellmanRegulariser}
    return RegulariserState(reg, NamedTuple())
end

function regularise(state::RegulariserState{NullBellmanRegulariser}, args...)
    return 0f0
end
####################################################################
